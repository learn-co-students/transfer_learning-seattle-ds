{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator,\\\n",
    "array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind transfer learning is simply to make use of some pre-trained model to make predictions, rather than building a new model from scratch.\n",
    "\n",
    "The plain fact is that several very powerful image-processing networks have already been built and perfected by scientists who have detailed knowledge about how all the layers of their models work. Moreover, many successful models have been trained on hundreds of thousands if not millions of images, and so they could be used for your images as well.\n",
    "\n",
    "In general, the target will of course be different from the original that was used in training the model in the first place. But the idea is that the model will be good at picking up on the *deep features* of images, and so we can use *most* of the pre-trained model, in order to extract those deep features, and then just stick on a couple extra layers at the end that are appropriate for the data we have.\n",
    "\n",
    "In what follows here we'll try building a network from scratch on some chest X-ray data. And then we'll see if we can get better accuracy by using [Imagenet](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/), a leading CNN for image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some X-rays of lungs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = './chest_xray/train/'\n",
    "test_f = './chest_xray/test/'\n",
    "val_f = './chest_xray/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras's ImageDataGenerator can convert images (we have JPEGs here) to tensors of visual information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_f, \n",
    "        target_size=(64, 64))\n",
    "\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_f, \n",
    "        target_size=(64, 64))\n",
    "\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_f, \n",
    "        target_size=(64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageDataGenerator uses *data augmentation*, which means that it will take each image and transform it in various ways, ultimately using *only these transformations* as training data. [Here's](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/) a nice resource on keras's `ImageDataGenerator`.\n",
    "\n",
    "And [here](https://bair.berkeley.edu/blog/2019/06/07/data_aug/) is a page with more information about data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1015 10:58:51.419598 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1015 10:58:51.435205 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1015 10:58:51.438150 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1015 10:58:51.454560 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1015 10:58:51.530231 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1015 10:58:51.545088 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1015 10:58:51.551514 4450960832 deprecation.py:323] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1015 10:58:53.768422 4450960832 deprecation_wrapper.py:119] From /Users/gdamico/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 0.7028 - acc: 0.2500 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6959 - acc: 0.3750 - val_loss: 0.6944 - val_acc: 0.4844\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6894 - acc: 0.6875 - val_loss: 0.6966 - val_acc: 0.4375\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6833 - acc: 0.7500 - val_loss: 0.6990 - val_acc: 0.4375\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6774 - acc: 0.7500 - val_loss: 0.7016 - val_acc: 0.4375\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6718 - acc: 0.7500 - val_loss: 0.7044 - val_acc: 0.4375\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6664 - acc: 0.7500 - val_loss: 0.7074 - val_acc: 0.4375\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6612 - acc: 0.7500 - val_loss: 0.7106 - val_acc: 0.4375\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6562 - acc: 0.7500 - val_loss: 0.7139 - val_acc: 0.4375\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6514 - acc: 0.7500 - val_loss: 0.7174 - val_acc: 0.4375\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6466 - acc: 0.7500 - val_loss: 0.7210 - val_acc: 0.4375\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6420 - acc: 0.7500 - val_loss: 0.7250 - val_acc: 0.4375\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6375 - acc: 0.7500 - val_loss: 0.7291 - val_acc: 0.4375\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6332 - acc: 0.7500 - val_loss: 0.7336 - val_acc: 0.4375\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6290 - acc: 0.7500 - val_loss: 0.7384 - val_acc: 0.4375\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6250 - acc: 0.7500 - val_loss: 0.7434 - val_acc: 0.4375\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6211 - acc: 0.7500 - val_loss: 0.7488 - val_acc: 0.4375\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6172 - acc: 0.7500 - val_loss: 0.7545 - val_acc: 0.4375\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6136 - acc: 0.7500 - val_loss: 0.7601 - val_acc: 0.4375\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6104 - acc: 0.7500 - val_loss: 0.7657 - val_acc: 0.4375\n"
     ]
    }
   ],
   "source": [
    "history_log = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the acc and val_acc scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with Transfer Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool comes from the [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(64, 64, 3)\n",
    "                )\n",
    "\n",
    "cnn_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size=batch_size, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(train_f, 5216) \n",
    "validation_features, validation_labels = extract_features(val_f, 16) \n",
    "test_features, test_labels = extract_features(test_f, 624)\n",
    "\n",
    "train_features = np.reshape(train_features, (5216, 2048))\n",
    "validation_features = np.reshape(validation_features, (16, 2048))\n",
    "test_features = np.reshape(test_features, (624, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216, 2048)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=2048))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5216 samples, validate on 624 samples\n",
      "Epoch 1/20\n",
      "5216/5216 [==============================] - 2s 297us/step - loss: 0.2218 - acc: 0.9112 - val_loss: 1.0821 - val_acc: 0.7115\n",
      "Epoch 2/20\n",
      "5216/5216 [==============================] - 1s 256us/step - loss: 0.1395 - acc: 0.9438 - val_loss: 1.3907 - val_acc: 0.6843\n",
      "Epoch 3/20\n",
      "5216/5216 [==============================] - 1s 228us/step - loss: 0.1282 - acc: 0.9482 - val_loss: 0.5981 - val_acc: 0.8237\n",
      "Epoch 4/20\n",
      "5216/5216 [==============================] - 1s 248us/step - loss: 0.1125 - acc: 0.9567 - val_loss: 1.3986 - val_acc: 0.6939\n",
      "Epoch 5/20\n",
      "5216/5216 [==============================] - 1s 280us/step - loss: 0.1111 - acc: 0.9584 - val_loss: 0.6225 - val_acc: 0.8237\n",
      "Epoch 6/20\n",
      "5216/5216 [==============================] - 2s 290us/step - loss: 0.1038 - acc: 0.9617 - val_loss: 0.8248 - val_acc: 0.7548\n",
      "Epoch 7/20\n",
      "5216/5216 [==============================] - 1s 267us/step - loss: 0.0988 - acc: 0.9661 - val_loss: 1.1544 - val_acc: 0.7388\n",
      "Epoch 8/20\n",
      "5216/5216 [==============================] - 1s 250us/step - loss: 0.0938 - acc: 0.9664 - val_loss: 1.9791 - val_acc: 0.6971\n",
      "Epoch 9/20\n",
      "5216/5216 [==============================] - 1s 239us/step - loss: 0.0984 - acc: 0.9641 - val_loss: 0.9669 - val_acc: 0.7676\n",
      "Epoch 10/20\n",
      "5216/5216 [==============================] - 1s 240us/step - loss: 0.0889 - acc: 0.9653 - val_loss: 1.0919 - val_acc: 0.7548\n",
      "Epoch 11/20\n",
      "5216/5216 [==============================] - 1s 246us/step - loss: 0.0882 - acc: 0.9711 - val_loss: 1.1306 - val_acc: 0.7612\n",
      "Epoch 12/20\n",
      "5216/5216 [==============================] - 1s 258us/step - loss: 0.0829 - acc: 0.9695 - val_loss: 1.2859 - val_acc: 0.7532\n",
      "Epoch 13/20\n",
      "5216/5216 [==============================] - 1s 249us/step - loss: 0.0813 - acc: 0.9691 - val_loss: 1.2276 - val_acc: 0.7500\n",
      "Epoch 14/20\n",
      "5216/5216 [==============================] - 1s 264us/step - loss: 0.0799 - acc: 0.9724 - val_loss: 1.3214 - val_acc: 0.7404\n",
      "Epoch 15/20\n",
      "5216/5216 [==============================] - 1s 259us/step - loss: 0.0770 - acc: 0.9724 - val_loss: 1.3955 - val_acc: 0.7564\n",
      "Epoch 16/20\n",
      "5216/5216 [==============================] - 1s 271us/step - loss: 0.0773 - acc: 0.9699 - val_loss: 1.6447 - val_acc: 0.7276\n",
      "Epoch 17/20\n",
      "5216/5216 [==============================] - 1s 264us/step - loss: 0.0724 - acc: 0.9741 - val_loss: 1.0407 - val_acc: 0.8061\n",
      "Epoch 18/20\n",
      "5216/5216 [==============================] - 1s 269us/step - loss: 0.0732 - acc: 0.9747 - val_loss: 1.4813 - val_acc: 0.7308\n",
      "Epoch 19/20\n",
      "5216/5216 [==============================] - 1s 259us/step - loss: 0.0697 - acc: 0.9749 - val_loss: 1.0201 - val_acc: 0.7837\n",
      "Epoch 20/20\n",
      "5216/5216 [==============================] - 1s 243us/step - loss: 0.0677 - acc: 0.9768 - val_loss: 2.0724 - val_acc: 0.6939\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "What other networks are available inside keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Use transfer learning with another pre-trained CNN on these data.\n",
    "# See if you can improve on our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
